{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/NLP-Course4Humanities_2024/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6ukO3QesXby"
      },
      "source": [
        "# Text Classification Using Small and Middle-sized LLMs\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png)](https://orcid.org/0000-0002-1031-2759)\n",
        "\n",
        "Text classification assigns predefined categories/labels to text documents. Example: Categorizing emails as spam/not-spam, or news articles into topics like sports, politics, technology.\n",
        "\n",
        "* **Classifying Earthquake Reports as an Example**\n",
        "\n",
        "As an text classification example, we use a dataset containing earthquake reporting and  classify the articles as either \"aid reports\" or \"others.\" This classification would - for example - enable further research into how humanitarian assistance is covered in disaster media coverage, providing insights into the prominence and patterns of aid-related reporting during crises.\n",
        "\n",
        "* **Trying Small and Middle-Sized LLMs**\n",
        "\n",
        "While fine-tuning language models for specific classification tasks can achieve strong results, not all research projects have the resources to fine-tune models. Using LLMs with in-context-learning can provide an accessible alternative by leveraging pre-trained knowledge through prompts, using few-shot learning with examples, and adapting to new tasks without parameter updates.  \n",
        "For this example, we compare a small (7b) and a middle-sized (70b) model for the same classification taks.\n",
        "\n",
        "* **Evaluating Model Output**\n",
        "\n",
        "In order to evaluate which model performs better, we need to compare the model results to ground truth results. Ground truth answers are verified, correct labels used as a reference point to evaluate model performance. They represent the \"true\" or \"correct\" classification determined by human experts or established criteria.\n",
        "Our dataset therefore also contains a column containing human ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the Dataset\n"
      ],
      "metadata": {
        "id": "J6ER2Rob2JTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opnGSEpDrtrr"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ieg-dhr/NLP-Course4Humanities_2024.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pLyOtJ7sJUC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "articles_df = pd.read_excel('/content/NLP-Course4Humanities_2024/datasets/earthquake_articles_examples.xlsx')\n",
        "\n",
        "articles_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification using the Qwen2.5 7b Model\n",
        "\n",
        "We'll access the model through Hugging Face's repository, which means it gets downloaded to our Colab runtime environment. It is recommended to switch to Colab's T4 GPU runtime for faster processing speed. Initial download and model processing will require approximately 10 minutes."
      ],
      "metadata": {
        "id": "J77fQz6w7yjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SAhS3UxSMOrp"
      },
      "outputs": [],
      "source": [
        "!pip install optimum\n",
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5TOreh1y7Rc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\",\n",
        "    device_map=\"balanced\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "def format_with_examples(text):\n",
        "    prompt = \"\"\"Klassifiziere den Text als entweder \"Hilfsreport\" oder \"Andere\". Setze deine Antwort in XML-Tags.\n",
        "Kriterien für \"Hilfsreport\":\n",
        "\n",
        "Spendenbeträge und deren Verwendung\n",
        "Aktivitäten von Hilfsorganisationen\n",
        "Anzahl der Helfer und Art der Hilfsleistungen\n",
        "Verteilung von Hilfsgütern\n",
        "Hilfskomitees und ihre Arbeit\n",
        "Hilfsmaßnahmen jeglicher Art\n",
        "Güter oder Geld für Katastrophenopfer\n",
        "Unterstützung für Opfer\n",
        "Hilfe und Wiederaufbau\n",
        "\n",
        "Überprüfe deine Antwort durch Kontrolle der relevanten Themen und Wörter sowie der Beispiele. Reflektiere deine Antwort und gib nur die beste Antwort.\n",
        "\n",
        "Example 1: Die Mannschaften der Kriegsschiffe haben an der Küste Schutzhütten fertig gestellt.\n",
        "<answer>Andere</answer>\n",
        "\n",
        "Example 2: Das deutsche Hilfskomitee sammelt Spenden. Kleidungs- und Wäschestücke werden geschickt.\n",
        "<answer>Hilfsreport</answer>\n",
        "\n",
        "Example 3: W Madrid, 11. Jan. (Telegr.) Der Finanzminister hat in der Kammer den Antrag eingebracht, für die Opfer des Erdbebens in Süditalien 200000 Pesetas zu bewilligen.\n",
        "<answer>Hilfsreport</answer>\n",
        "\n",
        "Classifiziere diesen Text:\n",
        "{text}\n",
        "\"\"\"\n",
        "    return prompt.format(text=text)\n",
        "\n",
        "def classify_text(text):\n",
        "    torch.cuda.empty_cache()\n",
        "    prompt = format_with_examples(text)\n",
        "    response = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=20,  # Increased for XML tags\n",
        "        do_sample=False,\n",
        "        temperature=0.1,\n",
        "        return_full_text=False\n",
        "    )[0]['generated_text'].strip()\n",
        "\n",
        "    # Extract text between XML tags\n",
        "    try:\n",
        "        import re\n",
        "        match = re.search(r'<answer>(.*?)</answer>', response)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        else:\n",
        "            return \"Andere\"  # Default fallback\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting response: {e}\")\n",
        "        return \"Andere\"\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "classifications = []\n",
        "for i in range(0, len(articles_df), batch_size):\n",
        "    batch = articles_df['extracted_article_clean'].iloc[i:i+batch_size]\n",
        "    for text in batch:\n",
        "        try:\n",
        "            classification = classify_text(text)\n",
        "            classifications.append(classification)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text: {e}\")\n",
        "            classifications.append(\"Error\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "articles_df['classification_qwen7b'] = classifications\n",
        "articles_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QqVGVo0HZ3I"
      },
      "outputs": [],
      "source": [
        "# Export to Excel\n",
        "articles_df.to_excel('classified_articles_qwen7b.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification Using the Nvidia/Llama 3.1 Nemotron 70b model\n",
        "\n",
        "The nemotron model is accessed via NVIDIA's API using our pre-configured authentication token stored in Colab's environment variables. The processing time will be considerably faster than with the HuggingFace model."
      ],
      "metadata": {
        "id": "ZKthheEFA2L4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDa0mJ0xfayB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=userdata.get('NVIDIA_TOKEN')\n",
        ")\n",
        "\n",
        "classifications = []\n",
        "for index, row in articles_df.iterrows():\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'system',\n",
        "                    'content': \"\"\"You are an expert in classification tasks\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': f\"\"\"Klassifiziere den Text als entweder \"Hilfsreport\" oder \"Andere\". Setze deine Antwort in XML-Tags.\n",
        "Kriterien für \"Hilfsreport\":\n",
        "\n",
        "Spendenbeträge und deren Verwendung\n",
        "Aktivitäten von Hilfsorganisationen\n",
        "Anzahl der Helfer und Art der Hilfsleistungen\n",
        "Verteilung von Hilfsgütern\n",
        "Hilfskomitees und ihre Arbeit\n",
        "Hilfsmaßnahmen jeglicher Art\n",
        "Güter oder Geld für Katastrophenopfer\n",
        "Unterstützung für Opfer\n",
        "Hilfe und Wiederaufbau\n",
        "\n",
        "Überprüfe deine Antwort durch Kontrolle der relevanten Themen und Wörter sowie der Beispiele. Reflektiere deine Antwort und gib nur die beste Antwort.\n",
        "\n",
        "Example 1: Die Mannschaften der Kriegsschiffe haben an der Küste Schutzhütten fertig gestellt.\n",
        "<answer>Andere</answer>\n",
        "\n",
        "Example 2: Das deutsche Hilfskomitee sammelt Spenden. Kleidungs- und Wäschestücke werden geschickt.\n",
        "<answer>Hilfsreport</answer>\n",
        "\n",
        "Example 3: W Madrid, 11. Jan. (Telegr.) Der Finanzminister hat in der Kammer den Antrag eingebracht, für die Opfer des Erdbebens in Süditalien 200000 Pesetas zu bewilligen.\n",
        "<answer>Hilfsreport</answer>\n",
        "\n",
        "Classifiziere diesen Text:\n",
        "\n",
        "{row['extracted_article_clean']}\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=20\n",
        "        )\n",
        "\n",
        "        content = completion.choices[0].message.content\n",
        "\n",
        "        try:\n",
        "            import re\n",
        "            match = re.search(r'<answer>(.*?)</answer>', content)\n",
        "            classifications.append(match.group(1).strip() if match else \"Andere\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting response for row {index}: {e}\")\n",
        "            classifications.append(\"Andere\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text for row {index}: {e}\")\n",
        "        classifications.append(\"Error\")\n",
        "\n",
        "articles_df['classification_nemotron70b'] = classifications\n",
        "articles_df.to_excel('classified_articles.xlsx', index=False)\n",
        "articles_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLPkhhxXNcv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1hKb-YflcCq"
      },
      "outputs": [],
      "source": [
        "# Export to Excel\n",
        "articles_df.to_excel('classified_articles_nemotron70b.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Comparison of the Model Performance\n",
        "\n",
        "The code below compares two classification models' performance on a dataset. It calculates overall accuracy and class-specific accuracies for \"Hilfsreport\" and \"Andere\" categories, then creates a bar chart using matplotlib showing three metrics per model: total accuracy (blue), Andere accuracy (green), and Hilfsreport accuracy (red). The visualization includes percentage labels on bars and requires a DataFrame with ground truth and model prediction columns."
      ],
      "metadata": {
        "id": "1fJNNKKwBIO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akVrktpcZH48"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_models_percentage(df, ground_truth_col, model1_col, model2_col=None):\n",
        "   # Calculate accuracies\n",
        "   acc1 = accuracy_score(df[ground_truth_col], df[model1_col]) * 100\n",
        "\n",
        "   def class_accuracies(y_true, y_pred):\n",
        "       hilfs_acc = sum((y_true == 'Hilfsreport') & (y_pred == 'Hilfsreport')) / sum(y_true == 'Hilfsreport') * 100\n",
        "       andere_acc = sum((y_true == 'Andere') & (y_pred == 'Andere')) / sum(y_true == 'Andere') * 100\n",
        "       return andere_acc, hilfs_acc\n",
        "\n",
        "   andere_acc1, hilfs_acc1 = class_accuracies(df[ground_truth_col], df[model1_col])\n",
        "\n",
        "   if model2_col:\n",
        "       acc2 = accuracy_score(df[ground_truth_col], df[model2_col]) * 100\n",
        "       andere_acc2, hilfs_acc2 = class_accuracies(df[ground_truth_col], df[model2_col])\n",
        "       models = ['Qwen7b', 'Nemotron70b']\n",
        "       total_acc = [acc1, acc2]\n",
        "       andere_acc = [andere_acc1, andere_acc2]\n",
        "       hilfs_acc = [hilfs_acc1, hilfs_acc2]\n",
        "   else:\n",
        "       models = ['Nemotron70b']\n",
        "       total_acc = [acc1]\n",
        "       andere_acc = [andere_acc1]\n",
        "       hilfs_acc = [hilfs_acc1]\n",
        "\n",
        "   x = np.arange(len(models))\n",
        "   width = 0.25\n",
        "\n",
        "   fig, ax = plt.subplots(figsize=(10, 6))\n",
        "   ax.bar(x - width, total_acc, width, label='Total Accuracy', color='blue')\n",
        "   ax.bar(x, andere_acc, width, label='Andere Accuracy', color='green')\n",
        "   ax.bar(x + width, hilfs_acc, width, label='Hilfsreport Accuracy', color='red')\n",
        "\n",
        "   ax.set_ylabel('Accuracy (%)')\n",
        "   ax.set_title('Model Comparison')\n",
        "   ax.set_xticks(x)\n",
        "   ax.set_xticklabels(models)\n",
        "   ax.legend()\n",
        "\n",
        "   def add_labels(bars):\n",
        "       for bar in bars:\n",
        "           height = bar.get_height()\n",
        "           ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                  f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "   for container in ax.containers:\n",
        "       add_labels(container)\n",
        "\n",
        "   plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# For one model:\n",
        "#compare_models_percentage(articles_df, 'ground_truth', 'classification_nemotron70b')\n",
        "# For two models:\n",
        "compare_models_percentage(articles_df, 'ground_truth', 'classification_qwen7b', 'classification_nemotron70b')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
