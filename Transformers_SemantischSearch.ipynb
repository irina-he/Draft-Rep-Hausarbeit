{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ieg-dhr/NLP-Course4Humanities_2024/blob/main/Transformers_SemantischSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformers and Semantic Search\n",
        "\n",
        "Created by Sarah Oberbichler [![ORCID](https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png)](https://orcid.org/0000-0002-1031-2759)\n",
        "\n",
        "*   Semantic search is a search engine technology that interprets the meaning of words and phrases. The results of a semantic search will return content matching the meaning of a query, as opposed to content that literally matches words in the query.\n",
        "*   Semantic search uses context clues to determine the meaning of a word across a dataset of millions of examples.\n",
        "Semantic search also identifies what other words can be used in similar contexts."
      ],
      "metadata": {
        "id": "EoR6mbZbIZoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ieg-dhr/NLP-Course4Humanities_2024.git"
      ],
      "metadata": {
        "id": "AhQgb4DtIXb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT8tlHq8yf_k"
      },
      "outputs": [],
      "source": [
        "# @markdown #### Let's import the dataset \"NorddeutscheZeitung_1909\"\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "df = pd.read_excel('/content/NLP-Course4Humanities_2024/datasets/NorddeutscheZeitung_1909.xlsx')\n",
        "\n",
        "# Now you can work with the DataFrame 'df'\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTkLifSjzHi6"
      },
      "outputs": [],
      "source": [
        "# @markdown #### Installing the Sentence Tranfromers from HuggingFace\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Transformers to find similar words\n",
        "\n",
        "Findig similar words is one of the most basic tasks for semantic search. While still operating on keywords, transformer models help us identify words with similar or related meanings, allowing us to broaden the scope of traditional keyword searches. This approach helps find relevant content even when exact keyword matches aren't present, by including semantically related terms in the search process.\n",
        "\n",
        "This code below implements a semantic word similarity search using the multilingual LaBSE transformer model (https://huggingface.co/sentence-transformers/LaBSE), where vector embeddings are generated dynamically based on context, unlike static word embeddings from older methods like Word2Vec. It processes a text corpus by converting all text to lowercase and removing special characters.\n",
        "The core functionality uses the transformer model to convert words into vector embeddings, then calculates cosine similarity between a target word (in this case \"Naturkatastrophen\") and all other filtered words from the corpus.\n",
        "\n",
        "Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them."
      ],
      "metadata": {
        "id": "xH7fZoCBKYnE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr5GVuxmzXNA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zäöüß\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def get_unique_words(text):\n",
        "    words = text.split()\n",
        "    return list(set(words))\n",
        "\n",
        "def find_similar_words(df, target_word, model, top_n=40):\n",
        "    # Preprocess the text\n",
        "    df['processed_text'] = df['plainpagefulltext'].apply(preprocess_text)\n",
        "\n",
        "    # Get unique words from all texts\n",
        "    all_words = []\n",
        "    for text in df['processed_text']:\n",
        "        all_words.extend(get_unique_words(text))\n",
        "\n",
        "    # Get unique words and their frequencies\n",
        "    word_freq = Counter(all_words)\n",
        "    unique_words = list(word_freq.keys())\n",
        "\n",
        "    print(f\"Number of unique words: {len(unique_words)}\")\n",
        "\n",
        "    # Encode the target word and unique words\n",
        "    target_embedding = model.encode([target_word])\n",
        "    word_embeddings = model.encode(unique_words)\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(target_embedding, word_embeddings)[0]\n",
        "\n",
        "    # Create a DataFrame with words and their similarities\n",
        "    word_sim_df = pd.DataFrame({\n",
        "        'word': unique_words,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    # Sort by similarity and get top N results\n",
        "    top_similar = word_sim_df.sort_values('similarity', ascending=False).head(top_n)\n",
        "\n",
        "    return top_similar\n",
        "\n",
        "# Load the pre-trained multilingual model\n",
        "print(\"Loading the sentence transformer model...\")\n",
        "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "target_word = \"Naturkatastrophen\"\n",
        "\n",
        "print(f\"\\nFinding words similar to '{target_word}'...\")\n",
        "similar_words = find_similar_words(df, target_word, model)\n",
        "\n",
        "print(\"\\nMost similar words:\")\n",
        "print(similar_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Intepended Search\n",
        "\n",
        "Unlike traditional keyword search, which would only find exact matches of words like \"earthquake\" or \"reconstruction,\" semantic search understands the conceptual meaning of the entire query. For example, when searching for \"reconstruction after earthquake,\" the system understands this as a concept involving disaster recovery, rebuilding efforts, community restoration, and infrastructure repair. This means it can identify relevant content that discusses these themes using different terminology - perhaps an article about \"community revival following seismic damage\" or \"rebuilding homes in disaster-struck areas.\" The search works by transforming both the query and the searchable content into mathematical representations (vectors) that capture their meaning in a multidimensional space, where similar concepts cluster together regardless of the specific words used to express them. This allows for a more intuitive and human-like understanding of language, capturing context, synonyms, related concepts, and even cross-language connections, ultimately providing more relevant and comprehensive search results that align with the user's actual information needs.\n",
        "\n"
      ],
      "metadata": {
        "id": "iGEv5pyLeRQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Let's import the dataset \"earthquake_articles\"\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_excel_file.xlsx' with the actual path to your Excel file\n",
        "articles_df = pd.read_excel('/content/NLP-Course4Humanities_2024/datasets/earthquake_articles.xlsx')\n",
        "\n",
        "# Now you can work with the DataFrame 'df'\n",
        "articles_df"
      ],
      "metadata": {
        "id": "XLLvNBsBSbbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below implements a semantic search functionality using a multilingual transformer model to find relevant articles for a specific query in a dataset. It first processes and cleans the input data, then uses the SBERT (Sentence-BERT) model 'paraphrase-multilingual-MiniLM-L12-v2' to convert both the search query and all articles into numerical vectors (embeddings) that capture their semantic meaning. Using cosine similarity, it then calculates how closely each article matches the search query, assigns similarity scores, and filters out articles with scores below 0.6. The results are sorted by similarity score in descending order, and the code outputs the top 10 most relevant articles, displaying their similarity scores, titles, and the first 600 characters of their content, making it easy to identify articles that are semantically related to the reconstruction after earthquakes theme, even if they don't contain the exact search terms."
      ],
      "metadata": {
        "id": "VgoPUikEhixM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuD6YXU-Louv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Load the transformer model\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Query phrase for semantic search\n",
        "query = \"Opfer durch Erdbeben\"\n",
        "\n",
        "# Preprocess DataFrame: drop rows where 'extracted_article_clean' is NaN or not a string\n",
        "if 'extracted_article_clean' not in articles_df.columns:\n",
        "    raise ValueError(\"The column 'extracted_article_clean' is missing in the DataFrame.\")\n",
        "\n",
        "# Drop rows with missing values in 'extracted_article_clean'\n",
        "articles_df = articles_df.dropna(subset=['extracted_article_clean'])\n",
        "\n",
        "# Ensure all entries in 'extracted_article_clean' are strings\n",
        "articles_df.loc[:, 'extracted_article_clean'] = articles_df['extracted_article_clean'].astype(str)\n",
        "\n",
        "# Encode the 'extracted_article_clean' column from the DataFrame\n",
        "article_embeddings = model.encode(articles_df['extracted_article_clean'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Encode the query\n",
        "query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# Calculate cosine similarity between query and each article\n",
        "similarities = util.pytorch_cos_sim(query_embedding, article_embeddings)[0]\n",
        "\n",
        "# Add similarity scores to the DataFrame\n",
        "articles_df = articles_df.copy()  # Avoid potential chained assignment warnings\n",
        "articles_df.loc[:, 'similarity'] = similarities.cpu().numpy()\n",
        "\n",
        "# Filter articles with similarity score > 0.6\n",
        "high_similarity_df = articles_df[articles_df['similarity'] > 0.6].copy()\n",
        "\n",
        "# Sort by similarity in descending order\n",
        "high_similarity_df = high_similarity_df.sort_values('similarity', ascending=False)\n",
        "\n",
        "# Print the number of articles found with similarity > 0.6\n",
        "print(f\"\\nFound {len(high_similarity_df)} articles with similarity score > 0.6\")\n",
        "\n",
        "# Print the top 10 highest similarity scores as examples\n",
        "print(\"\\nTop 10 highest similarity scores:\")\n",
        "top_10_examples = high_similarity_df.head(10)\n",
        "for _, row in top_10_examples.iterrows():\n",
        "    print(f\"\\nSimilarity Score: {row['similarity']:.4f}\")\n",
        "    print(f\"Title: {row['paper_title']}\")\n",
        "    print(f\"First 1000 characters of article: {row['extracted_article_clean'][:1000]}...\")\n",
        "\n",
        "# Save the filtered DataFrame to a new variable\n",
        "filtered_df = high_similarity_df\n",
        "\n",
        "print(f\"\\nShape of filtered DataFrame: {filtered_df.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN1WtWePGvG5sQltsiLssyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
